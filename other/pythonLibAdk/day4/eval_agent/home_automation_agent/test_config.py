import json

# Create evaluation configuration with basic criteria
eval_config = {
    "criteria": {
        "tool_trajectory_avg_score": 1.0,  # Perfect tool usage required
        "response_match_score": 0.8,  # 80% text similarity threshold
    }
}

with open("home_automation_agent/test_config.json", "w") as f:
    json.dump(eval_config, f, indent=2)

print("âœ… Evaluation configuration created!")
print("\nðŸ“Š Evaluation Criteria:")
print("â€¢ tool_trajectory_avg_score: 1.0 - Requires exact tool usage match")
print("â€¢ response_match_score: 0.8 - Requires 80% text similarity")
print("\nðŸŽ¯ What this evaluation will catch:")
print("âœ… Incorrect tool usage (wrong device, location, or status)")
print("âœ… Poor response quality and communication")
print("âœ… Deviations from expected behavior patterns")